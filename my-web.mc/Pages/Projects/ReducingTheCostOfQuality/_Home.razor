@page "/projects/reducing-the-cost-of-quality"

<PageTitle>Projects - Reducing the Cost of Quality</PageTitle>

<BlogWrapper>
    <BlogHeader Title='Reducing the Cost of Quality'
                Subtitle='Predictive Analytics in Regression Analysis'
                Date='May 2019'
                Tag='Professional' />
    <BlogParagraph>
        There's a certain kind of beauty found in mathematically representing the seemingly coincidental but often very explainable
        relationships between two or more things. Building these models is known as regression.
    </BlogParagraph>
    <BlogParagraph>
        Regression, a term familiar to many, remains underutilized in my workplace. When problems arise, the instinct is to delve into
        the data, seeking a deeper understanding by any means necessary. However, regression analysis is rarely performed, and even more
        infrequently do we discover meaningful explanatory models that shed light on how the issue originated. As any data enthusiast will
        confirm, it's not uncommon to conduct numerous analyses - regression or otherwise - only to uncover a small fraction of truly valuable
        insights.
    </BlogParagraph>
    <BlogParagraph>
        If modeling is a two-faced coin and explanatory modeling is on one side of this coin then <strong>predictive modeling</strong> is
        on the other. Like the faces of a coin, the two branches of modeling are backed by a similar core with a key difference between the two
        being which direction they are facing. Without going too much into the details, as this is not the purpose of this paper, explanatory
        modeling uses relationships to explain the past while predictive modeling uses relationships to try and predict the future or the unknown.
        Finally, the purpose of this paper is to introduce the concept of using predictive modeling as a form of dimensional measurement,
        to examine some criteria for evaluating the sufficiency of the models, and to discuss some methods for effectively developing an
        efficient subset of models that meets the established criteria for acceptance when hundreds of features are in play.
    </BlogParagraph>
    <BlogSubheader>
        Assumptions
    </BlogSubheader>
    <BlogParagraph>
        Throughout this paper, the primary measuring device and the actual measurements will be in the context of a Coordinate Measuring
        Machine (CMM), the quantity of features will be in the range of 150 to 250 - for sake of simplicity let's just say 200, and the
        product measured will be left ambiguous.
    </BlogParagraph>
    <BlogSubheader>
        The Value of Inspection
    </BlogSubheader>
    <BlogParagraph>
        I've been through two Lean Six Sigma course and currently hold a green belt certification. In each of these courses, my instructor
        touched on the subject of value added and non-value added activities and claimed that inspection fell within the non-value added
        category because it is not something that the customer is paying for.
    </BlogParagraph>
    <BlogQuote Author='Craig Borysowich'>
        Organizations may view inspection type activities as valuable activities. However, from the customer's viewpoint it is wasteful.
    </BlogQuote>
    <BlogParagraph>
        I struggled with this idea because in my business, it seems as though the customer is paying for the inspection: it was considered
        during the quoting process and the customer has requirements around it. However, recently I read an article,
        <NavLink href='https://it.toolbox.com/blogs/craigborysowich/determining-the-value-added-activities-of-a-process-112107'>
            Determining
            the 'Value-Added' Activities of a Process
        </NavLink> by Craig Borysowich, in which the author describes an equation for calculating
        the value a process: <strong>ValueOfProcess = ValueAfterProcess - ValueBeforeProcess</strong>.
    </BlogParagraph>
    <BlogParagraph>
        Regardless of the results of inspection, the value of each product is left unchanged; it either had or did not have a defect before
        the inspection. The difference between before and after inspection is our confidence in the product and so our focus perhaps should
        be increasing confidence in the product to eliminate, or at least minimize, this truly non-value added activity.
    </BlogParagraph>
    <BlogSubheader>
        Reducing the Cost of Inspection
    </BlogSubheader>
    <BlogParagraph>
        Two common methods in which my business reduce inspection costs are moving inspections from 100% to sampling based on historical
        evidence of sufficient performance and manufacturing alternative measuring devices. The benefit of these options are clear (a
        reduced cost of a non-value added activity) but each also has subtle indirect costs associated with it.
    </BlogParagraph>
    <BlogParagraph>
        Sampling is inherently risky as it comes with no guarantee. Historically good performance does not guarantee a continuing good
        performance and when an inevitable issue is observed during a periodic inspection, at least three costs accrue:
        <ol>
            <li>
                all parts not inspected between the previous sample and latest sample are in question and expensive engineering hours must be
                spent to evaluate the impact,
            </li>
            <li>
                all product after the point of inspect must return to the inspect operation and often new unplanned inspection must be
                introduced to the process going forward to continue shipping product with a new, again unexpected, source of non-conformance,
                and
            </li>
            <li>
                the very limited historical data significantly increases the difficulty of finding the source of the issue; all actions
                within the gap between samples is in question.
            </li>
        </ol>
    </BlogParagraph>
    <BlogParagraph>
        On the other hand, an alternative measuring device carries with it many costs of its own:
        <ol>
            <li>
                the cost of manufacturing an alternative gauge that can measure a few characteristics is already significant; as
                the complexity increases, one alternative, purchasing another CMM, becomes very competitive.
            </li>
            <li>
                one benefit of gauges is that they can provide immediate feedback for an operator. Features that can be adjusted in-line
                benefit greatly by this but there are proportionally very few features that in this category.
            </li>
            <li>
                in my experience, it is also rare for a gauge to be general enough to be applicable for more than a single product
                line and so the often untracked and indirect cost of inventorying and maintaining the many gauges can build quickly.
            </li>
        </ol>
    </BlogParagraph>
    <BlogParagraph>
        It is for these reasons that there exists opportunity for an alternative to improve upon some of these problems and predictive modeling
        may be this improved alternative. In comparison, predictive modeling improves upon sampling by providing the same benefit of lower cost
        inspection via a reduced inspection time but it exceeds sampling by doing so while continuing to measure each feature. Predictive
        modeling improves upon other alternative measuring devices by simply reducing the up-front cost and maintenance costs.
    </BlogParagraph>
    <BlogSubheader>
        Predictive Modeling as a Form of Measurement
    </BlogSubheader>
    <BlogParagraph>
        When the measured value of a feature is determined at least in part by another measurable feature, an opportunity for predictive
        modeling exists. Similarly, when the measured values of two features are determined at least in part by another feature, again,
        an opportunity for predictive modeling exists. A key fact of predictive modeling is that the predictor's, or independent variable's,
        value may be used to predict another feature's value. In this application, the prediction needs to be accurate enough to satisfy the
        requirement for confidence in the product.
    </BlogParagraph>
    <BlogParagraph>
        Typically, the requirements for qualifying an alternative form of measurement are defined for my business by a document
        called, <NavLink href='https://www.sae.org/standards/content/as13003/'>"Measurement Systems Analysis Requirements for the Aero Engine Supply Chain" (AS13003)</NavLink>. As I have not been able to find a
        publicly available release of this document, I will not go into detail but I will say that the document generally agrees with regard to
        alternative measurement with the generally accepted requirements:
        <ul>
            <li>
                correlation to within 10% of the tolerance,
            </li>
            <li>
                <NavLink href='https://www.spcforexcel.com/knowledge/measurement-systems-analysis/acceptance-criteria-for-MSA'>Repeatability & Reproducibility (R&R)</NavLink> result of 30% or less, depending on the feature.
            </li>
        </ul>
    </BlogParagraph>
    <BlogParagraph>
        The correlation requirement is one that we need to examine closely while <strong>
            the R&R requirement is one that we can completely
            disregard
        </strong>. Given a fixed input variable, operator A and operator B can use the model any number of times in any order and
        the result will never change therefore the R&R requirement is met when the device measuring the input variable has met it.
    </BlogParagraph>
    <BlogImage Source='projects/reducing-the-cost-of-quality/correlation_example.jpg'
               Alt='Figure 1' />
    <BlogParagraph>
        Regarding the correlation requirement, if the standard correlation requirement is applied, then the model is sufficient when the
        actual measurements of new data are within a ±10% of tolerance band around the model or, alternatively stated, the residuals of
        the model are within ±10% of the feature's tolerance.
    </BlogParagraph>
    <BlogParagraph>
        Accepting this criteria, the challenge then becomes optimization: to develop a method for identifying the smallest subset of
        sufficient models for predicting the assumed 200 features of the product. For this task, I turn to <NavLink href='https://www.r-project.org/'>R</NavLink>.
    </BlogParagraph>
    <BlogSubheader>
        Efficiently Evaluating the Models
    </BlogSubheader>
    <BlogParagraph>
        Considering a product with 200 features, it would not be efficient to evaluate individually the C(200, 2) = 19,900 possible linear
        models that exist and this is without even considering the possibility that multi-linear regression could hold some additional
        optimization opportunity (in fact, the potential added benefits of multi-linear regression will be a topic for a later write-up). The
        secret to developing and evaluating these models quickly relies on the power of present-day computing technology and an understanding
        of some basic statistics.
    </BlogParagraph>
    <BlogParagraph>
        We would like to show that the model predictions are sufficient enough that a high majority of all actual measurements are within 10%
        of the tolerance of the predicted value. This can be done by evaluating
        the <NavLink href='https://www.statisticshowto.datasciencecentral.com/rmse/'>Root Mean Square Error (RMSE)</NavLink>. For the remainder of this
        paper, I will assume that 95% of measurements satisfying the requirement is enough for a model to be considered sufficient; the
        remaining 5% of residuals will almost entirely remain within 15% of the tolerance. The RMSE is the standard deviation of the residuals
        and since any good model has a normally distributed set of residuals, a 95% confidence interval is one in which 1.96 standard deviations
        fit between the mean and the closest tolerance band. Since these models will be using a Gaussian best fit, the mean of the residuals will
        always be centered at zero and so saying that two standard deviations must fit between the mean and the closest tolerance band is the same
        as saying that the standard deviation must be equal to half of the tolerance band; in this case, the tolerance band is simply the 10% of
        the overall tolerance. Therefore, a model is sufficient when the RMSE is less than 5% of the overall
        tolerance. <NavLink href='https://www.statisticshowto.datasciencecentral.com/rmse/'>
            RMSE can be calculated using readily available values
            according to the following statement
        </NavLink>:
    </BlogParagraph>

    <BlogMath Value="RMSE = s_y \sqrt{1-R^2} " />
    
    <BlogParagraph>
        where s is the standard deviation of the predicted feature, y, and R-squared is the coefficient of determination.
    </BlogParagraph>
    <CodeWrapper Lines="@rcode.Split(Environment.NewLine)" />
    <BlogParagraph>
        Although the code above uses a nonsensical data set as a placeholder, when a proper data set and tolerances are supplied
        the resulting table identifies which row-features can be adequately predicted by a single variable linear function of which
        column-features and thus becomes a very valuable tool for determining the optimal subset of models.
    </BlogParagraph>
    <BlogParagraph>
        Before calculating the optimal subset there remains one more question to be considered.
    </BlogParagraph>
    <BlogSubheader>
        Is Transitive Modeling Acceptable?
    </BlogSubheader>
    <BlogParagraph>
        A <NavLink href='https://en.wikipedia.org/wiki/Transitive_relation'>transitive property</NavLink> is one such that if a implies b and b implies c then a implies c. In this context, the question is meant to ask that if a is sufficient to predict b and b is sufficient to predict c then when a is not sufficient on its own to predict c can the prediction of b from a be used to predict c? In a similar situation, if a is sufficient to predict c but has a greater RMSE to c than b should a model from b to c using the predicted values of b be used in place of the weaker relationship?

    </BlogParagraph>
    <BlogParagraph>
        The answer to both of these questions is simply no. Using the predicted value of b in either situation is effectively the same as allowing a to predict c as the composition of linear equations reduces to a linear equation.

    </BlogParagraph>
    <CodeWrapper Lines="@([
        "f(X) = mX + b",
        "g(Z) = nZ + c",
        "",
        "f(g(Z)) = m(nZ + c) + b",
        "        = mnZ + mc + b",
        "#this is the form of mX+b, a linear equation."
      ])">
    </CodeWrapper>
    <BlogParagraph>
        This fact implies that if a could have predicted c any better, it would have been represented by the model's RMSE.
    </BlogParagraph>
    <BlogSubheader>
        Finding the Optimal Subset
    </BlogSubheader>
    <BlogParagraph>
        An optimal subset is one that uses the least amount of measured features to adequately represent the whole product. If some features take significantly more time or effort to measure then the goal would be to minimize the overall time but for this application we will only seek to minimize the quantity of independent features.

    </BlogParagraph>
    <BlogParagraph>
        Developing an optimal algorithm will be a greater challenge but for now an algorithm that works can be outlined as:
        <ol>
            <li>Select the features that have no sufficient predictors as independent variable - dependent variable pairs (f(x) = x),</li>
            <li>If any, identify the features that are sufficiently predicted by the currently selected independent variables as dependent variables,</li>
            <li>Order the unselected variables by descending count of sufficient models for predicting unrepresented variables,</li>
            <li>Select the top independent variable as an independent variable and dependent variable,</li>
            <li>From the models selected in step 3 of the independent variable step 4, identify predicted features as dependent variables,</li>
            <li>Repeat steps 3 and 5 until all features are represented.</li>
        </ol>
    </BlogParagraph>
    <BlogParagraph>
        Independent variables are measured and dependent variables are calculated. When all dependent and independent features are identified, select each dependent feature's model according to which independent feature offers the least RMSE from the original full set of all models.
    </BlogParagraph>
    <BlogSubheader>
        Application Results
    </BlogSubheader>
    <BlogParagraph>
        Applying the algorithm on the product for which this idea was birthed, the result was a two-fold gain in efficiency for every 5% relaxation in correlation requirement from 10% to 20%. At 10%, the efficiency is approximately 200% (1:1 ratio of calculated CMM features to measured CMM features). Essentially, some features predict a lot of other features and many features do not satisfy the highly rigorous requirement and so these features had to be measured directly but the gains for slightly and reasonably relaxing the correlation requirement show immense gains. On a product with over 200 features required to be measured, the results were:
        <ul>
            <li>10% Correlation requirement &gt; 190% efficiency, approximately</li>
            <li>15% Correlation requirement &gt; 380% efficiency, approximately</li>
            <li>20% Correlation requirement &gt; 760% efficiency, approximately</li>
        </ul>
    </BlogParagraph>
    <BlogSubheader>
        Conclusion
    </BlogSubheader>
    <BlogParagraph>
        Reducing cost will always be a primary focus in the manufacturing environment. With high volumes, the benefits of reducing even a minute from a consistently timed task builds to a substantial and reliable cost reduction. The predictive modeling for measuring method however is not simply a cost savings effort. Additionally, it is an effort to allow better visibility of product performance under the guise of an inarguable truth: the direct cost savings makes it worth it. Strapped onto these direct cost savings are the indirect benefits of no data gaps that will allow analysis to be completed more thoroughly, root causes to be investigated more quickly, and customer requirements to be validated more readily.
    </BlogParagraph>
    <BlogParagraph>
        Compared to alternative-measurement cost-reduction options like sampling or the manufacturing of an alternative measuring device, predictive sampling improves upon some of the fatal flaws by reducing the risk and reducing the manufacturing and maintenance costs associated with each, respectively.
    </BlogParagraph>
    <BlogParagraph>
        Predictive modeling is not intended to replace these other options, as there are situations in which these options are ideal. Instead it is intended to amend the inspection-cost savings options by adding an additional method: a method that does not rely on the good condition of the product or on a high-cost tool.
    </BlogParagraph>
    <BlogSubheader>
        Looking forward
    </BlogSubheader>
    <BlogParagraph>
        This paper focuses entirely on single-variable linear models and is intended to be a gentle introduction for those unfamiliar with modeling by using very approachable concepts. It is possible that additional optimizations will be found by allowing more complex modeling options but the complexity of the algorithms will likely increase with the complexity of the models.
    </BlogParagraph>
</BlogWrapper>

@code {
        string rcode = 
            @"####
            ##  This algorithm should take in a matrix of 
            ##  numerical values and a supporting tolerance vector
            ##  and output the subset of models necessary to 
            ##  optimally and sufficiently represent all features.

            ## Input variables
            mydf = mtcars
            tol = c(20,20,1000,1000,10,20,6,6,6,4,6)
            corr.req = .1
            accuracy.req = .95
            zscore.of.accuracy.req = abs(qnorm((1-accuracy.req)/2))
            rmse.req = corr.req*tol/zscore.of.accuracy.req

            ## sd of features and r-sq and RMSE of linaer relationships
            sd.vector <- apply(mydf, 2, sd)
            coef.of.determ <- round(cor(mydf)**2,2)

            RMSE <- matrix(nrow = nrow(coef.of.determ), ncol = nrow(coef.of.determ))
            for (i in 1:ncol(coef.of.determ)){
                RMSE[,i] <- sd.vector*(1-coef.of.determ[,i])**.5
            }
            colnames(RMSE) <- colnames(coef.of.determ)
            rownames(RMSE) <- rownames(coef.of.determ)

            ## returns a boolean matrix identifying sufficiently 
            ## good linear relationships
            RMSE < rmse.req";
}
